apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-config
  namespace: default
data:
  server.json: |
    {
      "server": {
        "host": "0.0.0.0",
        "port": 8080,
        "worker_threads": 4,
        "request_timeout_ms": 30000,
        "max_request_size_mb": 10
      },
      "inference": {
        "max_batch_size": 8,
        "batch_timeout_ms": 10,
        "enable_batching": true,
        "device": "cpu",
        "num_inference_threads": 2
      },
      "models": {
        "model_directory": "/app/models",
        "default_models": [
          {
            "name": "resnet50",
            "path": "/app/models/resnet50.pt",
            "type": "classification",
            "enabled": true,
            "max_batch_size": 16
          }
        ]
      },
      "logging": {
        "level": "info",
        "console_output": true
      },
      "metrics": {
        "enabled": true,
        "prometheus_port": 9090
      }
    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-engine
  namespace: default
  labels:
    app: inference-engine
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: inference-engine
      version: v1
  template:
    metadata:
      labels:
        app: inference-engine
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: inference-engine
          image: your-registry/inference-engine:latest
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9090
              name: metrics
              protocol: TCP
          env:
            - name: LOG_LEVEL
              value: "info"
            - name: WORKER_THREADS
              value: "4"
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
              readOnly: true
            - name: models-volume
              mountPath: /app/models
              readOnly: true
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "2000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
      volumes:
        - name: config-volume
          configMap:
            name: inference-config
        - name: models-volume
          persistentVolumeClaim:
            claimName: models-pvc
      restartPolicy: Always
      terminationGracePeriodSeconds: 30

---
apiVersion: v1
kind: Service
metadata:
  name: inference-service
  namespace: default
  labels:
    app: inference-engine
spec:
  selector:
    app: inference-engine
  ports:
    - name: http
      port: 80
      targetPort: 8080
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: models-pvc
  namespace: default
spec:
  accessModes:
    - ReadOnlyMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-ssd

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: inference-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
    - host: inference.yourdomain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: inference-service
                port:
                  number: 80

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: inference-engine
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
